{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #version == 1.2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import cells\n",
    "from models import Stack_Layers_Model\n",
    "import time\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############## Read data ##################\n",
    "train_feature_dir = './data/TIMIT/phn/train/mfcc/'\n",
    "train_label_dir = './data/TIMIT/phn/train/label/'\n",
    "test_feature_dir = './data/TIMIT/phn/test/mfcc/'\n",
    "test_label_dir = './data/TIMIT/phn/test/label/'\n",
    "# read data from local path \n",
    "# a list of feature, each one has shape [feature_num, time_step]\n",
    "train_feature_list = read_ndarray_from(train_feature_dir)\n",
    "# a list of label, each one has shape [label_num]\n",
    "train_label_list = read_ndarray_from(train_label_dir)\n",
    "test_feature_list = read_ndarray_from(test_feature_dir)\n",
    "test_label_list = read_ndarray_from(test_label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the train feature, type: <type 'list'>, length: 200, type of each element: <type 'numpy.ndarray'>\n",
      "for the train label, type: <type 'list'>, length: 200, type of each element: <type 'numpy.ndarray'>\n",
      "for the test feature, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n",
      "for the test label, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "############ have a glance of the preprocessed data ############\n",
    "\n",
    "# # only pick a part of the dataset for quick debug\n",
    "train_feature_list = train_feature_list[:200]\n",
    "train_label_list = train_label_list[:200]\n",
    "test_feature_list = test_feature_list[:100]\n",
    "test_label_list = test_label_list[:100]\n",
    "\n",
    "print(\"for the train feature, type: {}, length: {}, type of each element: {}\".format(type(train_feature_list), len(train_feature_list), type(train_feature_list[0])))\n",
    "print(\"for the train label, type: {}, length: {}, type of each element: {}\".format(type(train_label_list), len(train_label_list), type(train_label_list[0])))\n",
    "print(\"for the test feature, type: {}, length: {}, type of each element: {}\".format(type(test_feature_list), len(test_feature_list), type(test_feature_list[0])))\n",
    "print(\"for the test label, type: {}, length: {}, type of each element: {}\".format(type(test_label_list), len(test_label_list), type(test_label_list[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Define hyper-paramaters ########################\n",
    "class Argument(object):\n",
    "    def __init__(self):\n",
    "        self.max_epoch = 500\n",
    "        self.num_layer = 2\n",
    "        self.num_hidden = 128\n",
    "        self.num_featue = train_feature_list[0].shape[0]\n",
    "        self.num_class = 62       \n",
    "\n",
    "        \n",
    "        self.lr_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.max_timestep = get_max_timestep(train_feature_list, test_feature_list)\n",
    "        self.layer_norm = True #only available for LSTMCell\n",
    "        self.dropout = 0.1 # drouput is only used between input and the first hidden layer, \n",
    "                           # and last hidden layer and output layer\n",
    "        self.isTrain = True #set a tag to judge whethe is training, used for dropout\n",
    "        \n",
    "        self.cell_type = 'LSTMCell' #option: LSTMCell, RNNCell, GRUCell, HyperLSTMCell\n",
    "        self.model_type = 'bidirection' #option: unidirection, bidirection, resnet, highway, seq2seq\n",
    "        \n",
    "        self.model_dir = \"./model/\" #file path to store trained model\n",
    "        self.log_dir = \"./log/\" #file path to store logs\n",
    "        self.log_name = str(datetime.datetime.now())+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Build a model #################################\n",
    "args = Argument()\n",
    "#define input and output tensor\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, args.max_timestep, args.num_featue], name=\"intputs\")\n",
    "targets_idx = tf.placeholder(tf.int64)\n",
    "targets_val = tf.placeholder(tf.int32)\n",
    "targets_shape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(targets_idx, targets_val, targets_shape)\n",
    "seq_len = tf.placeholder(tf.int32, [args.batch_size], name=\"seq_len\")\n",
    "#stack multi-layer networks\n",
    "layers_model = Stack_Layers_Model(args, inputs, targets, seq_len)\n",
    "logits = layers_model.build_model() #the logits is a tensor with shape: [batch_size, max_timestep, num_class]\n",
    "\n",
    "logits = tf.transpose(logits, [1,0,2]) #time major, shape: [max_timestep, batch_size, num_class]\n",
    "\n",
    "\n",
    "#optimizer\n",
    "loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(args.lr_rate).minimize(cost)\n",
    "predictions = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of trainable params is 446270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-42d2043906a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 targets_shape: train_targets_shp, seq_len: train_seq_len}\n\u001b[1;32m     47\u001b[0m             _, train_cost, train_preditions, train_targets = sess.run([optimizer, cost, predictions, targets], \n\u001b[0;32m---> 48\u001b[0;31m                                                                       feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtrain_batch_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_edit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_preditions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"during training mode, batch: {}/{}, epoch: {}, PER: {}\"\u001b[0m\u001b[0;34m.\u001b[0m                  \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############# Start Trainging ####################\n",
    "batch_size = args.batch_size\n",
    "max_epoch = args.max_epoch\n",
    "\n",
    "#split dataset into serveral batches\n",
    "level = 'phn'\n",
    "\n",
    "# feature_list = train_feature_list + test_feature_list\n",
    "# label_list = train_label_list + test_label_list\n",
    "\n",
    "\n",
    "# (batch_list, _) = data_lists_to_batches(feature_list, label_list, batch_size, level)\n",
    "# train_batch_list = batch_list[:len(train_feature_list)]\n",
    "# test_batch_list = batch_list[len(test_feature_list):]\n",
    "\n",
    "# num_train_batch = len(train_batch_list)\n",
    "# num_test_batch = len(test_batch_list)\n",
    "# train_error_list = [] #define a list to store each epoch error in training\n",
    "# test_error_list = [] #define a list to store each epoch error in testing\n",
    "\n",
    "\n",
    "(train_batch_list, _) = data_lists_to_batches(train_feature_list, train_label_list, batch_size, level, args.max_timestep)\n",
    "num_train_batch = len(train_batch_list)\n",
    "train_error_list = [] #define a list to store each epoch error in training\n",
    "\n",
    "(test_batch_list, _) = data_lists_to_batches(test_feature_list, test_label_list, batch_size, level, args.max_timestep)\n",
    "num_test_batch = len(test_batch_list)\n",
    "test_error_list = [] #define a list to store each epoch error in testing\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    num_trainable_params = np.sum([np.prod(v.shape) for v in tf.trainable_variables()]) #count params\n",
    "    print(\"num of trainable params is {}\".format(num_trainable_params))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(max_epoch):\n",
    "        #train the neural network\n",
    "        start_time = time.time()\n",
    "        train_batch_error = np.zeros(num_train_batch) #define a ndarray to store each batch error\n",
    "        args.isTrain = True\n",
    "        for i in range(num_train_batch):\n",
    "            train_inputs, train_targets, train_seq_len = train_batch_list[i]\n",
    "            train_targets_idx, train_targets_val, train_targets_shp = train_targets\n",
    "            feed_dict = {\n",
    "                inputs: train_inputs, targets_idx: train_targets_idx, targets_val: train_targets_val,\n",
    "                targets_shape: train_targets_shp, seq_len: train_seq_len}\n",
    "            _, train_cost, train_preditions, train_targets = sess.run([optimizer, cost, predictions, targets], \n",
    "                                                                      feed_dict=feed_dict)\n",
    "            train_batch_error[i] = get_edit_distance([train_preditions.values], [train_targets.values], True, level)\n",
    "            print(\"during training mode, batch: {}/{}, epoch: {}, PER: {}\".\\\n",
    "                  format(i+1, num_train_batch, epoch+1, train_batch_error[i]))\n",
    "            \n",
    "        train_epoch_error = np.mean(train_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "        print(\"in train mode, epoch: {}/{}, PER: {:.2f}, time: {:.2f}s\".\\\n",
    "              format(epoch+1, max_epoch, train_epoch_error, train_time))\n",
    "        train_error_list.append(train_epoch_error)\n",
    "        \n",
    "        #test the neural network\n",
    "        test_batch_error = np.zeros(num_test_batch) #define a ndarray to store each batch error\n",
    "        start_time = time.time()\n",
    "        args.isTrain = False\n",
    "        for i in range(num_test_batch):\n",
    "            test_inputs, test_targets, test_seq_len = test_batch_list[i]            \n",
    "            test_targets_idx, test_targets_val, test_targets_shp = test_targets\n",
    "            feed_dict = {\n",
    "                inputs: test_inputs, targets_idx: test_targets_idx, targets_val: test_targets_val,\n",
    "                targets_shape: test_targets_shp, seq_len: test_seq_len}\n",
    "            test_cost, test_predictions, test_targets = sess.run([cost, predictions, targets], feed_dict=feed_dict)\n",
    "            test_batch_error[i] = get_edit_distance([test_predictions.values], [test_targets.values], True, level)\n",
    "            \n",
    "        test_epoch_error = np.mean(test_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "        print(\"in test mode, epoch: {}/{}, PER: {:.2f}, time: {:.2f}s\".format(epoch+1, max_epoch, test_epoch_error, test_time))\n",
    "        test_error_list.append(test_epoch_error)\n",
    "        print(\"test truth:\\n\"+output_to_sequence(test_targets))\n",
    "        print(\"test prediction:\\n\"+output_to_sequence(test_predictions))\n",
    "\n",
    "        ################ save model and log info ##################\n",
    "        \n",
    "        #store trained model and logs\n",
    "        model_dir = args.model_dir\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.mkdir(model_dir) #create a new folder if not exist\n",
    "        check_point_path = os.path.join(model_dir, \"model.ckpt\")\n",
    "        saver.save(sess, check_point_path, global_step=epoch)\n",
    "        print(\"Model has been saved in {}\".format(model_dir))\n",
    "        #store logs in local file\n",
    "        log_dir = args.log_dir\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.mkdir(log_dir) #create a new log directionary if not exist\n",
    "        #open a log file\n",
    "        log_name = args.log_name\n",
    "        with open(os.path.join(log_dir, log_name), 'a') as f:\n",
    "            f.write(\"for train mode, epoch: {}, PER: {}, run time: {}\\n\".format(epoch+1, train_epoch_error, train_time))\n",
    "            f.write(\"for test mode, epoch: {}, PER: {}, run time: {}\\n\".format(epoch+1, test_epoch_error, test_time))\n",
    "            f.write(\"=================================================\\n\")\n",
    "            print(\"log has been saved in {}\".format(log_name))\n",
    "        print(\"===========================================\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
